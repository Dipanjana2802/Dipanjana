{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.applications import VGG16\n",
    "from tensorflow.python.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing scikit-optimize for doing the hyper-parameter optimization\n",
    "pip install h5py scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "#from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our objective is to find the hyper-parametes that makes our neural network model build on top of \n",
    "# a VGG-16 model employing Transfer Learning Technique to perform best at classifying our dataset\n",
    "\n",
    "# We will optimise the following hyperparameters:\n",
    "# The learning-rate of the optimizer.\n",
    "# The number of fully-connected / dense layers.\n",
    "# The number of nodes for each of the dense layers.\n",
    "# Whether to use 'sigmoid' or 'relu' activation in all the layers.\n",
    "\n",
    "# We will use the Python package scikit-optimize (or skopt) for finding the best choices of these hyper-parameters. \n",
    "# Before we begin with the actual search for hyper-parameters, we first need to define the valid search-ranges or \n",
    "# search-dimensions for each of these parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search-dimension for the learning-rate\n",
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',name='learning_rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search-dimension for the number of dense layers in the neural network\n",
    "dim_num_dense_layers = Integer(low=1, high=5, name='num_dense_layers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search-dimension for the number of nodes for each dense layer\n",
    "dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search-dimension for the activation-function\n",
    "dim_activation = Categorical(categories=['relu', 'sigmoid'],\n",
    "                             name='activation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then combine all these search-dimensions into a single list\n",
    "# This is how skopt works internally on hyper-parameters\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "             dim_activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting default parameters for testing the optimisation funtion(to be used later)\n",
    "default_parameters = [1e-5, 1, 16, 'relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "train_dir = '/content/drive/My Drive/Keras/Practice/cats_and_dogs/train'\n",
    "test_dir = '/content/drive/My Drive/Keras/Practice/cats_and_dogs/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of input tensor being used by VGG-16 model\n",
    "pre_model = VGG16(include_top=True, weights='imagenet')\n",
    "input_shape = pre_model.layers[0].output_shape[1:3]\n",
    "# input_shape = (224, 224)    \n",
    "\n",
    "# Making use of Keras's data-generator function to input data to the neural network\n",
    "# Increasing the size of the input data set by data augmentation during the data input process\n",
    "datagen_train = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=180,\n",
    "      width_shift_range=0.1,\n",
    "      height_shift_range=0.1,\n",
    "      shear_range=0.1,\n",
    "      zoom_range=[0.9, 1.5],\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True,\n",
    "      fill_mode='nearest')   \n",
    "\n",
    "\n",
    "#Only rescaling is done for test dataset as increasing dataset size is not required for testset\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of input tensor being used by VGG-16 model\n",
    "pre_model = VGG16(include_top=True, weights='imagenet')\n",
    "input_shape = pre_model.layers[0].output_shape[1:3]\n",
    "# input_shape = (224, 224)    \n",
    "\n",
    "# Making use of Keras's data-generator function to input data to the neural network\n",
    "# Increasing the size of the input data set by data augmentation during the data input process\n",
    "datagen_train = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=180,\n",
    "      width_shift_range=0.1,\n",
    "      height_shift_range=0.1,\n",
    "      shear_range=0.1,\n",
    "      zoom_range=[0.9, 1.5],\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True,\n",
    "      fill_mode='nearest')   \n",
    "\n",
    "\n",
    "#Only rescaling is done for test dataset as increasing dataset size is not required for testset\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Following function takes a set of the hyperparameters we need to optimise and creates a neural network model through Transfer Learning principle \n",
    "def create_model(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, activation):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "    model = VGG16(include_top=True, weights='imagenet')\n",
    "    \n",
    "    # We have checked separately and found the last convolutional layer of VGG-16 model is called 'block5_pool'\n",
    "    # We refer to this layer as the Transfer Layer because its output will be re-routed to our new fully-connected neural network\n",
    "    # which will do the classification for our dataset\n",
    "    transfer_layer = model.get_layer('block5_pool')\n",
    "    \n",
    "    # Next is to create a new model using Keras API\n",
    "    # First we take the part of the VGG16 model from its input-layer to the output of the transfer-layer\n",
    "    conv_model = Model(inputs=model.input, outputs=transfer_layer.output)\n",
    "    \n",
    "    # Next we will build a new model on top of this\n",
    "    # Start a new Keras Sequential model.\n",
    "    new_model = Sequential()\n",
    "    \n",
    "    # Add the convolutional part of the VGG16 model from above.\n",
    "    new_model.add(conv_model)\n",
    "    \n",
    "    # Flatten the 4-rank output of the convolutional layers\n",
    "    # to 2-rank that can be input to a fully-connected / dense layer.\n",
    "    new_model.add(Flatten())\n",
    "\n",
    "    # Add fully-connected / dense layers.\n",
    "    # The number of layers is a hyper-parameter we want to optimize.\n",
    "    for i in range(num_dense_layers):\n",
    "        # Name of the layer. This is not really necessary\n",
    "        # because Keras should give them unique names.\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "\n",
    "        # Add the dense / fully-connected layer to the model.\n",
    "        # This has two hyper-parameters we want to optimize:\n",
    "        # The number of nodes and the activation function.\n",
    "        new_model.add(Dense(num_dense_nodes,\n",
    "                        activation=activation,\n",
    "                        name=name))\n",
    "\n",
    "    # Last fully-connected / dense layer with softmax-activation\n",
    "    # for use in classification.\n",
    "    new_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Use the Adam method for training the network.\n",
    "    # Our objective is to find the best learning-rate for the Adam method.\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    # In Transfer Learning we intend to reuse the pre-trained VGG16 model as it is, so we will disable training for all its layers\n",
    "\n",
    "    conv_model.trainable = False\n",
    "\n",
    "    for layer in conv_model.layers:\n",
    "      layer.trainable = False\n",
    "      \n",
    "    new_model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return new_model\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neural network with the best hyper-parameters is saved to disk so it can be reloaded later. This is the filename for the model\n",
    "path_best_model = 'Optimised_model.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the classification accuracy for the model saved to disk. \n",
    "# It is a global variable which will be updated during optimization of the hyper-parameters\n",
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function that creates and trains a neural network with the given hyper-parameters, \n",
    "# and then evaluates its performance on the validation-set. \n",
    "# The function then returns the so-called fitness value (aka. objective value), which is the classification accuracy on the validation-set\n",
    "\n",
    "# The function decorator @use_named_args wraps the fitness function so that it can be called with all the parameters as a single list\n",
    "# This is the calling-style skopt uses internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, num_dense_layers,\n",
    "            num_dense_nodes, activation):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = create_model(learning_rate=learning_rate,\n",
    "                         num_dense_layers=num_dense_layers,\n",
    "                         num_dense_nodes=num_dense_nodes,\n",
    "                         activation=activation)\n",
    "       \n",
    "    # Train the model    \n",
    "    history = model.fit_generator(generator=generator_train,\n",
    "                                  steps_per_epoch=steps_per_epoch,\n",
    "                                  class_weight=class_weight,\n",
    "                                  validation_data=generator_test,\n",
    "                                  validation_steps=steps_test)\n",
    "\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Run of the fitness function using default parameter set\n",
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=<number of calls to fitness()>,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the progress of the hyper-parameter optimization\n",
    "plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyper-Parameters\n",
    "# The best hyper-parameters found by the Bayesian optimizer are packed as a list because that is what it uses internally.\n",
    "\n",
    "search_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now as we have got the best set of hyperparameters we will train our model with these hyperparameters following transfer learning method\n",
    "model = VGG16(include_top=True, weights='imagenet')\n",
    "transfer_layer = model.get_layer('block5_pool')\n",
    "conv_model = Model(inputs=model.input, outputs=transfer_layer.output)\n",
    "\n",
    "new_model = Sequential()\n",
    "\n",
    "new_model.add(conv_model)\n",
    "new_model.add(Flatten())\n",
    "\n",
    "# Two intermediate dense layers with 167 nodes in each and 'relu' activation are being added to the network\n",
    "new_model.add(Dense(<identified optimised value for number of nodes>, activation='<identified activation function>'))\n",
    "\n",
    "new_model.add(Dense(<identified optimised value for number of nodes>, activation='<identified activation function>')) \n",
    "\n",
    "# Last fully-connected / dense layer with softmax-activation\n",
    "# for use in classification.\n",
    "new_model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "optimizer = Adam(lr=<identified optimised value for learning rate>)\n",
    "\n",
    "conv_model.trainable = False\n",
    "\n",
    "for layer in conv_model.layers:\n",
    "  layer.trainable = False    \n",
    "      \n",
    "new_model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = new_model.fit_generator(generator=generator_train,\n",
    "                                  epochs= epochs,\n",
    "                                  steps_per_epoch= steps_per_epoch,\n",
    "                                  class_weight=class_weight,\n",
    "                                  validation_data=generator_test,\n",
    "                                  validation_steps=steps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying classification accuracy:\n",
    "result = new_model.evaluate_generator(generator_test, steps=steps_test)\n",
    "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next is Fine Tuning\n",
    "\n",
    "# As the new classifier has been trained we can try and gently fine-tune some of the deeper layers in the VGG16 model through Fine Tuning\n",
    "# Thus we will find the best suited learning rate for fine tuning the model\n",
    "# Setting up search-dimension for the learning-rate for fine tuning\n",
    "dim_learning_rate = Real(low=1e-9, high=1e-7, prior='log-uniform',name='learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to setup fine-tuned model\n",
    "def create_model_fine_tuned(learning_rate):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    \"\"\"\n",
    "    # As we have our new classifier trained during transfer learning process, we can try and gently fine-tune\n",
    "    # some of the deeper layers in the VGG16 model through Fine Tuning\n",
    "    # We want to train the last two convolutional layers whose names contain 'block5' or 'block4'\n",
    "    conv_model.trainable = True\n",
    "\n",
    "    for layer in conv_model.layers:\n",
    "      # Boolean whether this layer is trainable.\n",
    "      trainable = ('block5' in layer.name or 'block4' in layer.name)\n",
    "    \n",
    "      # Set the layer's bool.\n",
    "      layer.trainable = trainable\n",
    "    \n",
    "    # Use the Adam method for training the network.\n",
    "    # We want to find the best learning-rate for the Adam method.\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    # Compiling the model once again \n",
    "    new_model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_dimensions = [dim_learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation function\n",
    "@use_named_args(dimensions=fine_tune_dimensions)\n",
    "def fitness_fine_tuned(learning_rate):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = create_model_fine_tuned(learning_rate=learning_rate)\n",
    "                            \n",
    "    # Train the model    \n",
    "    history = model.fit_generator(generator=generator_train,\n",
    "                                  steps_per_epoch=steps_per_epoch,\n",
    "                                  class_weight=class_weight,\n",
    "                                  validation_data=generator_test,\n",
    "                                  validation_steps=steps_test)\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save(path_best_model)\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_parameters_fine_tune = [1e-7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the actual hyper-parameter optimization\n",
    "search_result_fine_tuned = gp_minimize(func=fitness_fine_tuned,\n",
    "                            dimensions=fine_tune_dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=<number of calls to fitness()>,\n",
    "                            x0 = default_parameters_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the progress of the hyper-parameter optimization\n",
    "plot_convergence(search_result_fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the best value for the hyperparameter\n",
    "search_result_fine_tuned.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
