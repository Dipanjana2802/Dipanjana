Hyper Parameter Optimisation for Building an Optimum Neural Network for Specific Requirement

The process is built based on Bayesian optimization principle.

Bayesian optimization's main working principle is to construct another model (model used here is Gaussian Process) to evaluate a set of hyper-parameters. 
This estimates how the performance of a NN model varies with changes to the hyper-parameters. 
The NN model is being evaluated gain and again with new sets of hyper-parameters until the best set of parameters is identified by the Bayesian optimizer.

Hyper parameters optimised:
-  The number of fully-connected / dense layers of the NN
-  The number of nodes for each of the dense layers
-  The learning-rate of the optimizer used
-  The best suitable activation function to be used

Python package scikit-optimize (or skopt) is used to find the best choices or the best combination of the identified hyper-parameters.

Detailed code with step wise explanation is provided in Hyper_Parameter_Optimisation.py
